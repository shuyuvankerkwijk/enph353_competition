{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4c79c0-1e76-4968-ae5f-a3a6ee3161cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:01:49.411905: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb030a31-72a1-4370-8727-82fb3256b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:02:05.079987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.084019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.084136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.085173: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-02 19:02:05.086235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.086368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.086470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.354149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.354247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.354304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-04-02 19:02:05.354364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6116 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 19:02:06.347293: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2025-04-02 19:02:07.435249: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323/1323 [==============================] - 131s 98ms/step - loss: 0.1124 - mae: 0.2392 - val_loss: 0.5071 - val_mae: 0.5171 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.1060 - mae: 0.2300 - val_loss: 0.3022 - val_mae: 0.4125 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.1039 - mae: 0.2272 - val_loss: 0.2856 - val_mae: 0.4029 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.1022 - mae: 0.2244 - val_loss: 0.4110 - val_mae: 0.4753 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.1020 - mae: 0.2229 - val_loss: 0.6119 - val_mae: 0.5449 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.1015 - mae: 0.2227 - val_loss: 0.2232 - val_mae: 0.3262 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.1017 - mae: 0.2226 - val_loss: 0.4662 - val_mae: 0.4677 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0998 - mae: 0.2196 - val_loss: 0.3111 - val_mae: 0.4092 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0979 - mae: 0.2178 - val_loss: 0.6594 - val_mae: 0.5251 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0977 - mae: 0.2162 - val_loss: 0.3289 - val_mae: 0.4371 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0973 - mae: 0.2154 - val_loss: 0.3330 - val_mae: 0.4314 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0986 - mae: 0.2173 - val_loss: 0.4774 - val_mae: 0.4739 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "1322/1323 [============================>.] - ETA: 0s - loss: 0.0976 - mae: 0.2152\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0976 - mae: 0.2151 - val_loss: 0.2530 - val_mae: 0.3601 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0947 - mae: 0.2114 - val_loss: 0.6045 - val_mae: 0.5768 - lr: 5.0000e-05\n",
      "Epoch 15/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0947 - mae: 0.2118 - val_loss: 0.5958 - val_mae: 0.5511 - lr: 5.0000e-05\n",
      "Epoch 16/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0955 - mae: 0.2129 - val_loss: 0.0950 - val_mae: 0.2193 - lr: 5.0000e-05\n",
      "Epoch 17/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0955 - mae: 0.2124 - val_loss: 0.1716 - val_mae: 0.2994 - lr: 5.0000e-05\n",
      "Epoch 18/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0931 - mae: 0.2093 - val_loss: 0.3402 - val_mae: 0.4304 - lr: 5.0000e-05\n",
      "Epoch 19/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0937 - mae: 0.2105 - val_loss: 0.1545 - val_mae: 0.2773 - lr: 5.0000e-05\n",
      "Epoch 20/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0942 - mae: 0.2110 - val_loss: 0.1940 - val_mae: 0.3156 - lr: 5.0000e-05\n",
      "Epoch 21/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0936 - mae: 0.2100 - val_loss: 0.1894 - val_mae: 0.3175 - lr: 5.0000e-05\n",
      "Epoch 22/100\n",
      "1323/1323 [==============================] - 128s 97ms/step - loss: 0.0917 - mae: 0.2077 - val_loss: 0.2206 - val_mae: 0.3607 - lr: 5.0000e-05\n",
      "Epoch 23/100\n",
      "1322/1323 [============================>.] - ETA: 0s - loss: 0.0934 - mae: 0.2092\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0934 - mae: 0.2093 - val_loss: 0.7535 - val_mae: 0.5968 - lr: 5.0000e-05\n",
      "Epoch 24/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0924 - mae: 0.2084 - val_loss: 0.2052 - val_mae: 0.3225 - lr: 2.5000e-05\n",
      "Epoch 25/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0913 - mae: 0.2066 - val_loss: 0.1488 - val_mae: 0.2676 - lr: 2.5000e-05\n",
      "Epoch 26/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0926 - mae: 0.2085 - val_loss: 0.0916 - val_mae: 0.2014 - lr: 2.5000e-05\n",
      "Epoch 27/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0919 - mae: 0.2076 - val_loss: 0.1865 - val_mae: 0.2980 - lr: 2.5000e-05\n",
      "Epoch 28/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0910 - mae: 0.2067 - val_loss: 0.1619 - val_mae: 0.2778 - lr: 2.5000e-05\n",
      "Epoch 29/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0911 - mae: 0.2059 - val_loss: 0.0986 - val_mae: 0.2090 - lr: 2.5000e-05\n",
      "Epoch 30/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0918 - mae: 0.2067 - val_loss: 0.1771 - val_mae: 0.2927 - lr: 2.5000e-05\n",
      "Epoch 31/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0896 - mae: 0.2043 - val_loss: 0.2232 - val_mae: 0.3416 - lr: 2.5000e-05\n",
      "Epoch 32/100\n",
      "1323/1323 [==============================] - 129s 97ms/step - loss: 0.0913 - mae: 0.2059 - val_loss: 0.1023 - val_mae: 0.2166 - lr: 2.5000e-05\n",
      "Epoch 33/100\n",
      " 545/1323 [===========>..................] - ETA: 1:10 - loss: 0.0932 - mae: 0.2089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[0;32m---> 20\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     Section_histories\u001b[38;5;241m.\u001b[39mappend(history)\n\u001b[1;32m     23\u001b[0m plot_histories(Section_histories, Sections)\n",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, DATA_DIR, Section)\u001b[0m\n\u001b[1;32m      3\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m create_dataset(DATA_DIR)\n\u001b[1;32m      4\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m      6\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m ]\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "IMG_HEIGHT = 316\n",
    "IMG_WIDTH = 384\n",
    "BATCH_SIZE = 16\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "USE_EXISTING_MODEL = True\n",
    "\n",
    "Sections = [\"Road\"]\n",
    "\n",
    "Section_histories = []\n",
    "\n",
    "for Section in Sections:\n",
    "    DATA_DIR = '/home/fizzer/ros_ws/training_for_driving/' + Section + '/images' \n",
    "    if USE_EXISTING_MODEL:\n",
    "        model = tf.keras.models.load_model('/home/fizzer/ros_ws/training_for_driving/'+Section+'/best_model.h5')\n",
    "        model.optimizer.learning_rate.assign(1e-4)\n",
    "    else:\n",
    "        model = create_model()\n",
    "    history = Train(model, DATA_DIR, Section)\n",
    "    Section_histories.append(history)\n",
    "    \n",
    "plot_histories(Section_histories, Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858502b6-6ec4-48cf-adc8-dd94caf8f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(filename):\n",
    "    # Extract filename from path\n",
    "    filename_only = tf.strings.split(filename, os.path.sep)[-1]\n",
    "    \n",
    "    # Regex pattern to capture Lin and Ang values\n",
    "    pattern = r'.*_Lin_(-?\\d+\\.\\d{2})_Ang_(-?\\d+\\.\\d{2})\\.png$'\n",
    "    \n",
    "    # Extract values using regex replace and split\n",
    "    lin_ang_str = tf.strings.regex_replace(filename_only, pattern, r'\\1,\\2')\n",
    "    parts = tf.strings.split(lin_ang_str, ',')\n",
    "    \n",
    "    # Convert to floats\n",
    "    lin = tf.strings.to_number(parts[0], tf.float32)\n",
    "    ang = tf.strings.to_number(parts[1], tf.float32)\n",
    "    \n",
    "    return tf.stack([lin, ang])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29da9dc0-9bab-4624-ac41-f411040ee2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir):\n",
    "    # List all PNG files\n",
    "    ds = tf.data.Dataset.list_files(os.path.join(data_dir, \"*.png\"), shuffle=True)\n",
    "    \n",
    "    # Get cardinality as tensor\n",
    "    cardinality = tf.data.experimental.cardinality(ds)\n",
    "    \n",
    "    # Calculate split sizes using TensorFlow operations\n",
    "    val_size = tf.cast(\n",
    "        tf.cast(cardinality, tf.float32) * VAL_SPLIT,\n",
    "        tf.int64\n",
    "    )\n",
    "    train_size = cardinality - val_size\n",
    "\n",
    "    # Split dataset\n",
    "    train_ds = ds.skip(val_size)\n",
    "    val_ds = ds.take(val_size)\n",
    "\n",
    "    # Rest of the processing remains the same...\n",
    "    def process_path(file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.io.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img, parse_labels(file_path)\n",
    "\n",
    "    train_ds = train_ds.shuffle(10000).map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73cc203-01b8-41e2-89c0-4fbb58f7c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        \n",
    "        # Enhanced convolutional base\n",
    "        layers.Conv2D(64, (5, 5), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense layers with L2 regularization\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer='l2'),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "        layers.Dense(2, activation='tanh')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),  # Lower initial LR\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3da64b-d68b-44b7-84c6-b0ec71149a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(model, DATA_DIR, Section):\n",
    "\n",
    "    train_dataset, val_dataset = create_dataset(DATA_DIR)\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=50,  # Increased from 10\n",
    "            min_delta=0.00001,  # Minimum change to qualify as improvement\n",
    "            mode='min',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,  # Wait longer before reducing LR\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            '/home/fizzer/ros_ws/training_for_driving/'+Section+'/best_model.h5',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b992ef-5c57-4baa-9110-a29a2a26cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(histories, sections):\n",
    "    \"\"\"Dynamically plots loss and accuracy for multiple training histories.\"\"\"\n",
    "    \n",
    "    # Determine available metrics dynamically\n",
    "    all_metrics = set()\n",
    "    for history in histories:\n",
    "        if history is not None:\n",
    "            all_metrics.update(history.history.keys())\n",
    "\n",
    "    loss_metrics = [m for m in all_metrics if 'loss' in m]\n",
    "    acc_metrics = [m for m in all_metrics if 'accuracy' in m or 'acc' in m]\n",
    "\n",
    "    num_plots = len(loss_metrics) + len(acc_metrics)\n",
    "    \n",
    "    # Ensure at least one plot is created\n",
    "    if num_plots == 0:\n",
    "        print(\"No metrics found in histories.\")\n",
    "        return\n",
    "    \n",
    "    fig, axs = plt.subplots(num_plots, 1, figsize=(10, 5 * num_plots))\n",
    "    \n",
    "    # Convert axs to a list if only one subplot is created\n",
    "    if num_plots == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    plot_idx = 0\n",
    "    \n",
    "    # Plot all loss-related metrics\n",
    "    for loss_metric in loss_metrics:\n",
    "        for i, history in enumerate(histories):\n",
    "            if history is None:\n",
    "                continue\n",
    "            label = sections[i]\n",
    "            axs[plot_idx].plot(history.history[loss_metric], label=f\"{label} - {loss_metric}\")\n",
    "        axs[plot_idx].set_title(loss_metric.replace('_', ' ').title())\n",
    "        axs[plot_idx].set_ylabel(\"Loss\")\n",
    "        axs[plot_idx].set_xlabel(\"Epoch\")\n",
    "        axs[plot_idx].legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "    # Plot all accuracy-related metrics\n",
    "    for acc_metric in acc_metrics:\n",
    "        for i, history in enumerate(histories):\n",
    "            if history is None:\n",
    "                continue\n",
    "            label = sections[i]\n",
    "            axs[plot_idx].plot(history.history[acc_metric], label=f\"{label} - {acc_metric}\")\n",
    "        axs[plot_idx].set_title(acc_metric.replace('_', ' ').title())\n",
    "        axs[plot_idx].set_ylabel(\"Accuracy\")\n",
    "        axs[plot_idx].set_xlabel(\"Epoch\")\n",
    "        axs[plot_idx].legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34225ccd-b450-47b1-8618-9e71e0f34c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa4a7c-6203-479e-9751-b1669eebcaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
